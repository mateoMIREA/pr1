# Интеллектуальные системы и технологии

# Практическая работа №1

**Разработка алгоритма k близжайших соседей.**

**Тема — определение домашнего животного — кошты или собаки.**

---

## Данные объектов

- **Уровень физической активности** (занимаетесь ли вы активно спортом, более 2 часов в неделю) — *0 или 1*  
- **Количество времени, проводимые в день вне дома** — *часы от 0 до 24*  
- **Любите ли вы активный отдых** — *0/1*  
- **Любите ли вы пассивный отдых** — *0/1*  
- **Часто ли вы путешествуете** — *0/1* (нечасто/часто)  
- **Вы экставерт** — *0/1*  
- **Вы интроверт** — *0/1*  
- **Вы живете в загородном доме** — *0/1*  
- **Вы живете в квартире** — *0/1*  
- **Близко ли к вам есть парк** — *0/1*  
- **Есть ли у вас дети** — *0/1*  
- **Хотите ли вы любить** — *0/1*  
- **Хотите ли вы быть любимым** — *0/1*  
- **Какие ваши ежемесячные траты на домашнее животное** — *число от 0 до 100000*  
- **Вам комфортный уровень шума дБ** — *от 0 до 100*  
- **Сколько раз в неделю убираетесь в доме** — *число от 1 до 7*

### Наборы данных

- **`cat_dog.csv`** — чистые данные. Сгенерированы очень точно, что не показывает никакой ошибки в данных.  
- **`cat_dog_noisy.csv`** — есть шумовые данные, когда данные не всегда совпадают с результатом, поэтому присутствует ошибка.  
- **`cat_dog_noisy_300.csv`** — шумовые данные на 300 объектов. Показывают переобучение модели (чем больше *k*, тем хуже результат).


## Содержание
1. [Цель работы](#цель-работы)  
2. [Постановка задачи](#постановка-задачи)  
3. [Метод и метрики](#метод-и-метрики)  
4. [Ход эксперимента](#ход-эксперимента)  
     4.1 [Загрузка и первичный анализ](#41-загрузка-и-первичный-анализ)  
     4.2 [Подготовка данных](#42-подготовка-данных)  
     4.3 [Базовая модель (k=5)](#43-базовая-модель-k5)  
     4.4 [Поиск оптимального k](#44-поиск-оптимального-k)  
     4.5 [Визуализация границ решений (PCA→2D)](#45-визуализация-границ-решений-pca2d)  
5. [Результаты](#результаты)  
6. [Выводы](#выводы)

---

## Цель работы
Освоить принципы алгоритма **k-ближайших соседей** на задаче бинарной классификации «кошка/собака»: подготовить данные, обучить модель, подобрать гиперпараметр *k*, оценить качество и визуализировать границы решений.

## Постановка задачи
По вектору описанных выше признаков необходимо предсказать предпочтение питомца: класс `cat` или `dog`.

## Метод и метрики
- **Модель:** `KNeighborsClassifier` (евклидово расстояние, равные веса соседей).  
- **Предобработка:** `StandardScaler` (обязательна для методов, зависящих от расстояний).  
- **Разбиение:** обучающая/тестовая = **80/20** со стратификацией.  
- **Подбор гиперпараметра:** перебор `k ∈ {1,…,19}`.  
- **Метрики:** `Accuracy`, `Precision`, `Recall`, `F1` по каждому классу.
---

## Ход эксперимента

### 4.1 Загрузка и первичный анализ
Загружаем датасет, убеждаемся, что столбцы прочитались с корректными типами и нет критичных пропусков. Дополнительно смотрим первые строки (`head()`), чтобы визуально проверить диапазоны значений и соответствие признаков описанию.
![Загрузка и info](images/step-1-head-info.png)

### 4.2 Подготовка данных
- Разделяем данные на матрицу признаков **`X`** и целевую переменную **`y`**.  
- Для удобства визуализаций используем кодирование меток: `{'cat': 0, 'dog': 1}` (сам классификатор может работать и со строковыми метками).  
- Выполняем разбиение на обучающую и тестовую части: **80/20**, `random_state=42`, `stratify=y` (стратификация сохраняет доли классов).  
- Масштабируем признаки с помощью **`StandardScaler`**, так как k-NN использует расстояния и чувствителен к разным шкалам признаков.  
![Подготовка данных](images/step-2-prep.png)

### 4.3 Базовая модель (k=5)
Обучаем базовую модель **k-NN** с числом соседей **k = 5** на масштабированных признаках. После обучения считаем метрики качества на тестовой выборке.

**Результаты (тест):** `Accuracy ≈ 0.97`. Ниже приведены сводные метрики по классам и матрица ошибок.  
Интерпретация:
- Метрики **Precision/Recall/F1** близки по классам, что говорит о сбалансированном качестве.  
- В **матрице ошибок** малая величина вне диагонали означает немногочисленные ложные классификации.
 
![Код и метрики k=5](images/step-3-k5-metrics.png)
![Матрица ошибок k=5](images/step-3-k5-cm.png)

Пример сводной таблицы по отчёту классификации:

| Класс      | Precision | Recall | F1  | Support |
|------------|-----------|--------|-----|---------|
| Cat Lover  | 0.97      | 0.95   | 0.96| 41      |
| Dog Lover  | 0.96      | 0.98   | 0.97| 46      |
| **Accuracy** |           |        | **0.97** | **87** |

### 4.4 Поиск оптимального k
Для оценки влияния гиперпараметра **k** проводим перебор значений от **1** до **19**. Для каждого `k` обучаем модель на обучающей части и измеряем точность на тестовой.

- **Лучшее значение:** `k = 1` (точность ≈ 0.97).  
- Тенденция: при увеличении `k` граница решений сглаживается, что может ухудшать качество на данных с локальными структурами.

![Цикл подбора k](images/step-4-grid-k.png)
![График точности train/test vs k](images/step-4-acc-vs-k.png)
![Логи лучшего k](images/step-4-logs.png)

> **Замечание по датасету `cat_dog_noisy_300.csv`:**  
> при малых `k` возможны признаки локального переобучения к шуму, а при росте `k` наблюдается **падение тестовой точности** (пересглаживание), что подтверждает описание набора — «чем больше *k*, тем хуже результат».

### 4.5 Визуализация границ решений (PCA→2D)
Для наглядности признаки стандартизированы и понижены до 2 компонент методом PCA, затем обучен k-NN с найденным k=1.  
![Код PCA-визуализации](images/step-5-pca-code.png)
![Границы решений k=1 (PCA 2D)](images/step-5-pca-plot.png)

---

## Результаты
1. **Качество.** Базовая модель с `k=5` дала **Accuracy ≈ 0.97**; подбор подтвердил **оптимум k=1** (≈ 0.97).  
2. **Влияние шума.** В `cat_dog_noisy.csv` и особенно в `cat_dog_noisy_300.csv` шум снижает устойчивость k-NN; рост *k* приводит к пере-сглаживанию и потере качества.  
3. **Стандартизация.** Существенно улучшает сопоставимость признаков, поэтому обязательна для k-NN.  
4. **PCA-визуализация.** На 2D-проекции видны локально разделимые кластеры, что объясняет высокую точность при малых *k*.

## Выводы

- **Алгоритм k-NN подходит для задачи «кошка/собака».** На тесте получена точность порядка **0.97** при стандартизации признаков (на наборе `cat_dog_noisy_300.csv`), что подтверждает применимость метода к данным данного типа.
- **Оптимальное число соседей — `k = 1`.** Перебор `k ∈ [1; 19]` показал максимум качества при `k=1`; при `k=5` метрики сопоставимы, но слегка ниже.
- **Шум ухудшает устойчивость.** На «чистом» `cat_dog.csv` качество наивысшее; добавление шума (`cat_dog_noisy.csv`, `cat_dog_noisy_300.csv`) снижает стабильность предсказаний и повышает чувствительность к выбору `k`.
- **Эффект переобучения/пересглаживания.** Для `cat_dog_noisy_300.csv` при малых `k` границы решений становятся «рваными» (риски переобучения к шуму), а при росте `k` наблюдается **падение качества** из-за пересглаживания — модель теряет локальные различия классов (**чем больше `k`, тем хуже результат**).
- **Стандартизация признаков обязательна** для k-NN, так как метрика расстояний чувствительна к масштабу; без масштабирования метрики заметно хуже.

